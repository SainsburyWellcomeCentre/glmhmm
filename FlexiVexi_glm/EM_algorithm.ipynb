{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the EM algorithm to infer transition probabilities and GLM weights per state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The details of the model are very well described in the paper methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import GoalSelection.training_metrics as tm\n",
    "from pathlib import Path\n",
    "import glmhmm.glm as glm\n",
    "import glmhmm.glm_hmm as glm_hmm\n",
    "import FlexiVexi_glm.design_matrix as dm\n",
    "import FlexiVexi_glm.visualize as viz\n",
    "import glmhmm.utils as uti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = Path('/Volumes/sjones/projects/FlexiVexi/behavioural_data')\n",
    "MOUSE = 'FNT103'\n",
    "DATE = '2024-08-02'\n",
    "PORTS = [[0.6, 0.35], \n",
    "         [-0.6, 0.35], \n",
    "         [0, -0.7]]\n",
    "\n",
    "BIAS = True\n",
    "\n",
    "exp_data = tm.build_exp_data(MOUSE, DATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit mouse-wide GLM\n",
    "The main use of this is a sanity check of the data and making the mouse-wide design matrix, hence not  using a bias term. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, row_identity, design_concat = dm.design_matrix_per_mouse(MOUSE, -19, bias = BIAS)\n",
    "GLM = dm.build_GLM(design_concat, y)\n",
    "w_init =  GLM.init_weights()\n",
    "w, phi  = GLM.fit(X, w_init, y)\n",
    "fig, ax = viz.plot_model_weights(MOUSE, GLM, bias = BIAS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, use the GLMHMM class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_GLMHMM(design, y, states, observations='bernoulli', return_params = False):\n",
    "        \n",
    "    '''\n",
    "    c  is 2  in a  bernouilli choice. \n",
    "\n",
    "    n: number of data/time points\n",
    "    d: number of features (inputs to design matrix)\n",
    "    c: number of classes (possible observations)\n",
    "    x: design matrix (nxm)\n",
    "    y: observations (nxc)\n",
    "    w: weights mapping x to y (mxc or mx1)\n",
    "    '''\n",
    "    n = len(design)\n",
    "    d = len(design.columns)-1\n",
    "    if observations == 'bernoulli':\n",
    "        c = 2   \n",
    "    else:\n",
    "        print('Think about your number of observations!')\n",
    "\n",
    "    k = states\n",
    "\n",
    "    GLMHMM = glm_hmm.GLMHMM(n, d, c, k) \n",
    "\n",
    "    params = {'n':n, \n",
    "              'd':d, \n",
    "              'c':c, \n",
    "              'k':k}\n",
    "\n",
    "    if return_params:\n",
    "        return GLMHMM, params\n",
    "    else:\n",
    "        return GLMHMM\n",
    "\n",
    "GLMHMM, params = build_GLMHMM(design_concat, y, states = 3, return_params = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates parameters A, w, and pi for a GLM-HMM. Can be used to generate true parameters for simulated data\n",
    "or to initialize parameters for fitting. I don't see why it doesn't make sense to have a uniform prior here. \n",
    "\n",
    "Parameters:\n",
    "\n",
    "- weights : list, optional  \n",
    "    Contains the name of the desired distribution (string) and optionally the associated parameters \n",
    "    (see init_params.py script for details. The default is ['uniform',-1,1,1].  \n",
    "\n",
    "- transitions : list, optional  \n",
    "    Contains the name of the desired distribution (string). The default is ['dirichlet',5,1].  \n",
    "\n",
    "- state_priors : string, optional  \n",
    "    Containts the name of the desired distribution (string). The default is None, or 'uniform'.  \n",
    "    \n",
    "\n",
    "Returns:\n",
    "\n",
    "A : kxk matrix of transition probabilities.\n",
    "w : mxc matrix of weights.\n",
    "pi : kx1 vector of state probabilities for t=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, w, pi = GLMHMM.generate_params(weights=['uniform',-1,1,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first try to do 2 initializations and initialise the weights as a uniform distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inits = 2 # set the number of initializations\n",
    "\n",
    "# store values for each initialization\n",
    "lls_all = np.zeros((inits,250))\n",
    "A_all = np.zeros((inits,params['k'],params['k']))\n",
    "w_all = np.zeros((inits,params['k'],params['d'],params['c']))\n",
    "\n",
    "# fit the model for each initialization\n",
    "for i in range(inits):\n",
    "    A_init,w_init,pi_init = GLMHMM.generate_params() # initialize the model parameters\n",
    "    lls_all[i,:],A_all[i,:,:],w_all[i,:,:],pi0 = GLMHMM.fit(y,X,A_init,w_init) # fit the model\n",
    "    print('initialization %s complete' %(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestix = uti.find_best_fit(lls_all) # find the initialization that led to the best fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`w_all` is a tensor shaped initialisations x states x regressors x observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_end = w_all[bestix,  :, :, :]\n",
    "weights_end.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabels = [\n",
    "    'Cue identity',\n",
    "    'History of last choice 1',\n",
    "    'History of last choice 2',\n",
    "    'History of last choice 3',\n",
    "    'History of last choice 4',\n",
    "    'History of last choice 5',\n",
    "    'Last rewarded choice',\n",
    "    'Distance to 0',\n",
    "    'Distance to 1',\n",
    "    'bias'\n",
    "]\n",
    "fig, ax = plt.subplots(params['k'])\n",
    "for i in range(params['k']):\n",
    "    ax[i].plot(weights_end[i,:,:])\n",
    "\n",
    "ax[params['k']-1].set_xticks(np.arange(0,len(xlabels)))\n",
    "ax[params['k']-1].plot(xlabels,np.zeros((len(xlabels),1)),'k--')\n",
    "ax[params['k']-1].set_xticklabels(xlabels, rotation =  90)\n",
    "trials = params['n']\n",
    "states = params['k']\n",
    "fig.suptitle(f'GLMHMM weights for {MOUSE}, {trials} trials, {states} states')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It basically picks up spatial bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Being a bit smarter about the weights\n",
    "\n",
    "We will now use the GLM fit weights plus a bit of noise to explain the mouse behaviour. The generate_params method calls a function un init_params.py that employs as parameters:\n",
    "\n",
    "- A low and a high estimates to initialise  the glm weights (they are also approximated, not computer analytically)\n",
    "- The X and y of the complete GLM\n",
    "- A bias term. As far as I can see, bias is not used, but the system will add a bias term itself, so I should'nt do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_prior_params = ['GLM', -0.2, 1.2, X, y, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inits = 2 # set the number of initializations\n",
    "\n",
    "# store values for each initialization\n",
    "lls_all = np.zeros((inits,250))\n",
    "A_all = np.zeros((inits,params['k'],params['k']))\n",
    "w_all = np.zeros((inits,params['k'],params['d'],params['c']))\n",
    "\n",
    "# fit the model for each initialization\n",
    "for i in range(inits):\n",
    "    A_init,w_init,pi_init = GLMHMM.generate_params(weights = glm_prior_params) # initialize the model parameters\n",
    "    lls_all[i,:],A_all[i,:,:],w_all[i,:,:],pi0 = GLMHMM.fit(y,X,A_init,w_init) # fit the model\n",
    "    print('initialization %s complete' %(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_model_weights_states(MOUSE, w_all, lls_all, GLMHMM, bias=BIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glmhhmm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
